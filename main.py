import streamlit as st
import numpy as np
import tensorflow as tf
import tempfile
import os
import cv2
import mediapipe as mp
from scipy.interpolate import interp1d
import time
from pathlib import Path
import base64

from load_env import load_dotenv
from text_to_speech import synthesize_speech
st.set_page_config(page_title="VSL Prediction", layout="centered")
st.title("D·ª∞ ƒêO√ÅN NG√îN NG·ªÆ K√ù HI·ªÜU")

mp_holistic = mp.solutions.holistic
N_UPPER_BODY_POSE_LANDMARKS = 25
N_HAND_LANDMARKS = 21
N_TOTAL_LANDMARKS = N_UPPER_BODY_POSE_LANDMARKS + N_HAND_LANDMARKS + N_HAND_LANDMARKS

ALL_POSE_CONNECTIONS = list(mp_holistic.POSE_CONNECTIONS)
UPPER_BODY_POSE_CONNECTIONS = []
# ====================
# Load model v√† label_map
# ====================
@st.cache_resource
def load_model():
    return tf.keras.models.load_model('Models/checkpoints_gpu/best_model.keras')  # model ƒë√£ hu·∫•n luy·ªán

@st.cache_data
def load_label_map():
    import json
    with open('Logs/label_map.json', 'r', encoding='utf-8') as f:
        label_map = json.load(f)
    inv_label_map = {v: k for k, v in label_map.items()}
    return label_map, inv_label_map

load_dotenv()
model = load_model()
label_map, inv_label_map = load_label_map()
tts_output_dir = Path("Outputs/app_predictions")
# ====================
# H√†m x·ª≠ l√Ω video (placeholder)
# ====================
def mediapipe_detection(image, model):
    # Mediapipe d√πng RGB, cv2 d√πng BGR
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def extract_keypoints(results):
    pose_kps = np.zeros((N_UPPER_BODY_POSE_LANDMARKS, 3))
    left_hand_kps = np.zeros((N_HAND_LANDMARKS, 3))
    right_hand_kps = np.zeros((N_HAND_LANDMARKS, 3))
    if results and results.pose_landmarks:
        for i in range(N_UPPER_BODY_POSE_LANDMARKS):
            if i < len(results.pose_landmarks.landmark):
                res = results.pose_landmarks.landmark[i]
                pose_kps[i] = [res.x, res.y, res.z]
    if results and results.left_hand_landmarks:
        left_hand_kps = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])
    if results and results.right_hand_landmarks:
        right_hand_kps = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])
    keypoints = np.concatenate([pose_kps,left_hand_kps, right_hand_kps])
    return keypoints.flatten()

def interpolate_keypoints(keypoints_sequence, target_len = 60):#n·ªôi suy chu·ªói keypoints v·ªÅ 60 frames
    if len(keypoints_sequence) == 0:
        return None

    original_times = np.linspace(0, 1, len(keypoints_sequence))
    target_times = np.linspace(0, 1, target_len)

    num_features = keypoints_sequence[0].shape[0]
    interpolated_sequence = np.zeros((target_len, num_features))

    for feature_idx in range(num_features):
        feature_values = [frame[feature_idx] for frame in keypoints_sequence]

        interpolator = interp1d(
            original_times, feature_values,
            kind='cubic', #n·ªôi suy cubic
            bounds_error=False, #kh√¥ng b√°o l·ªói n·∫øu ngo√†i ph·∫°m vi
            fill_value="extrapolate" #ngo·∫°i suy n·∫øu c·∫ßn
        )
        interpolated_sequence[:, feature_idx] = interpolator(target_times)

    return interpolated_sequence

def sequence_frames(video_path, holistic):
  sequence_frames = []
  cap = cv2.VideoCapture(video_path)
  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

  step = max(1, total_frames // 100)  # x√°c ƒë·ªãnh b∆∞·ªõc nh·∫£y ƒë·ªÉ l·∫•y m·∫´u frames

  while cap.isOpened():#ƒë·ªçc t·ª´ng frame t·ª´ video
      ret, frame = cap.read()
      if not ret:
          break

      #n·∫øu kh√¥ng ph·∫£i frame c·∫ßn l·∫•y m·∫´u th√¨ b·ªè qua
      if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % step != 0:
          continue

      try:
          image, results = mediapipe_detection(frame, holistic)#d√πng mediapipe ƒë·ªÉ x√°c ƒë·ªãnh keypoints
          keypoints = extract_keypoints(results)#tr√≠ch xu·∫•t keypoints t·ª´ k·∫øt qu·∫£

          if keypoints is not None:
              sequence_frames.append(keypoints)

      except Exception as e:
          continue

  cap.release()
  return sequence_frames

def process_webcam_to_sequence():
    cap = cv2.VideoCapture(0)  # S·ª≠ d·ª•ng webcam m·∫∑c ƒë·ªãnh
    st.write("‚è≥ ƒêang chu·∫©n b·ªã... B·∫Øt ƒë·∫ßu trong 1.5 gi√¢y...")
    time.sleep(1.5)  # Hi·ªÉn th·ªã th√¥ng b√°o trong 1.5 gi√¢y
    
    # ƒê·ªçc video t·ª´ webcam trong 4 gi√¢y
    st.write("üé• ƒêang ghi h√¨nh trong 4 gi√¢y...")
    sequence = []
    start_time = time.time()

    # Kh·ªüi t·∫°o Mediapipe Holistic model
    holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
    stframe = st.empty()

    while True:
        ret, frame = cap.read()
        if not ret:
            st.error("Kh√¥ng th·ªÉ truy c·∫≠p webcam")
            break
        elapsed_time = time.time() - start_time
        if elapsed_time > 4:  # Sau 4 gi√¢y th√¨ d·ª´ng
            break
        # Chuy·ªÉn ƒë·ªïi frame t·ª´ BGR (OpenCV) sang RGB (Mediapipe)
        image, results = mediapipe_detection(frame, holistic)

        # Tr√≠ch xu·∫•t keypoints t·ª´ k·∫øt qu·∫£ c·ªßa Mediapipe
        keypoints = extract_keypoints(results)
        
        # Th√™m keypoints v√†o chu·ªói (c√≥ th·ªÉ d·ª´ng sau 60 frames ho·∫∑c khi ng∆∞·ªùi d√πng nh·∫•n n√∫t)
        if keypoints is not None:
            sequence.append(keypoints)

        # Hi·ªÉn th·ªã webcam feed tr√™n Streamlit
        stframe.image(image, channels="BGR", caption="Webcam feed", use_container_width=True)

    cap.release()
    
    return sequence

# Streamlit App

def autoplay_audio(audio_path: Path):
    mime = "audio/mpeg"
    if audio_path.suffix.lower() == ".wav":
        mime = "audio/wav"
    elif audio_path.suffix.lower() == ".ogg":
        mime = "audio/ogg"

    with open(audio_path, "rb") as f:
        audio_bytes = f.read()
    b64 = base64.b64encode(audio_bytes).decode()
    audio_html = f"""
        <audio autoplay>
            <source src="data:{mime};base64,{b64}" type="{mime}">
        </audio>
    """
    st.markdown(audio_html, unsafe_allow_html=True)

input_mode = st.radio("Ch·ªçn ngu·ªìn ƒë·∫ßu v√†o:", ["üéûÔ∏è Video file", "üì∑ Webcam"])

sequence = None
holistic =mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)
if input_mode == "üéûÔ∏è Video file":
    uploaded_file = st.file_uploader("T·∫£i l√™n video (.mp4, .avi)", type=["mp4", "avi"])
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
            tmp.write(uploaded_file.read())
            tmp_path = tmp.name
        st.video(tmp_path)
        if st.button("üîç D·ª± ƒëo√°n t·ª´ video"):
            sequence = sequence_frames(tmp_path, holistic)

elif input_mode == "üì∑ Webcam":
    st.warning("Nh·∫•n n√∫t b√™n d∆∞·ªõi ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi h√¨nh t·ª´ webcam.")
    if st.button("üì∏ Ghi v√† d·ª± ƒëo√°n"):
        sequence = process_webcam_to_sequence()

# D·ª± ƒëo√°n
if sequence is not None:
    kp = interpolate_keypoints(sequence)
    result = model.predict(np.expand_dims(kp, axis=0))
    pred_idx = np.argmax(result, axis=1)
    pred_label = [inv_label_map[idx] for idx in pred_idx]
    st.success(f"‚úÖ Nh√£n d·ª± ƒëo√°n: **{pred_label}**")

    if pred_label:
        recognized_text = pred_label[0]
        try:
            audio_file = tts_output_dir / f"prediction_{int(time.time())}.mp3"
            synthesize_speech(recognized_text, audio_file, voice="coral")
            autoplay_audio(audio_file)
            st.info("üîä VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c ph√°t ngay l·∫≠p t·ª©c.")
        except Exception as tts_error:
            st.warning(f"Kh√¥ng th·ªÉ ph√°t √¢m thanh TTS: {tts_error}")
